| distributed init (rank 0): tcp://localhost:17385
| distributed init (rank 2): tcp://localhost:17385
| distributed init (rank 3): tcp://localhost:17385
| distributed init (rank 1): tcp://localhost:17385
| initialized host r02g04.bullx as rank 2
| initialized host r02g04.bullx as rank 3
| initialized host r02g04.bullx as rank 0
| initialized host r02g04.bullx as rank 1
Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, arch='transformer', attention_dropout=0.0, bert_first=True, bert_gates=[1, 1, 1, 1, 1, 1], bert_model_name='/scratch/project_2003453/bert-base-finnish-uncased-v1', bert_output_layer=-1, bert_ratio=1.0, bucket_cap_mb=25, clip_norm=25, cpu=False, criterion='label_smoothed_cross_entropy', curriculum=0, data='/scratch/project_2003453/bertnmt_data', dataset_impl='cached', ddp_backend='c10d', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layers=6, decoder_learned_pos=False, decoder_no_bert=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method='tcp://localhost:17385', distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=4, dropout=0.3, encoder_attention_heads=8, encoder_bert_dropout=True, encoder_bert_dropout_ratio=0.5, encoder_bert_mixup=False, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_learned_pos=False, encoder_normalize_before=False, encoder_ratio=1.0, find_unused_parameters=False, finetune_bert=False, fix_batches_to_gpus=False, fp16=True, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, lazy_load=False, left_pad_source='True', left_pad_target='False', log_format=None, log_interval=1000, lr=[0.0005], lr_scheduler='inverse_sqrt', mask_cls_sep=False, max_epoch=0, max_sentences=None, max_sentences_valid=None, max_source_positions=1024, max_target_positions=1024, max_tokens=16000, max_update=150000, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=1e-09, no_epoch_checkpoints=False, no_progress_bar=False, no_save=False, no_token_positional_embeddings=False, num_workers=0, optimizer='adam', optimizer_overrides='{}', raw_text=False, required_batch_size_multiple=8, reset_dataloader=False, reset_lr_scheduler=True, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='/scratch/project_2003453/bertnmt_data/bert-nmt_fi_sv', save_interval=1, save_interval_updates=0, seed=1, sentence_avg=False, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, source_lang='fi', target_lang='sv', task='translation', tbmf_wrapper=False, tensorboard_logdir='', threshold_loss_scale=None, train_subset='train', update_freq=[1], upsample_primary=1, user_dir=None, valid_subset='valid', validate_interval=1, warmup_from_nmt=True, warmup_init_lr=1e-07, warmup_nmt_file='checkpoint_nmt.pt', warmup_updates=4000, weight_decay=0.0001)
| [fi] dictionary: 37752 types
| [sv] dictionary: 37752 types
| /scratch/project_2003453/bertnmt_data valid fi-sv 5000 examples
bert_gates [True, True, True, True, True, True]
TransformerModel(
  (encoder): TransformerEncoder(
    (embed_tokens): Embedding(37752, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (embed_tokens): Embedding(37752, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (bert_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (bert_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (bert_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (bert_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (bert_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (bert_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (bert_encoder): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(50101, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
)
| model transformer, criterion LabelSmoothedCrossEntropyCriterion
| num. model params: 195863040 (num. trained: 71344128)
| training on 4 GPUs
| max tokens per GPU = 16000 and max sentences per GPU = None
Model will load checkpoint from /scratch/project_2003453/bertnmt_data/bert-nmt_fi_sv/checkpoint_nmt.pt
| loaded checkpoint /scratch/project_2003453/bertnmt_data/bert-nmt_fi_sv/checkpoint_nmt.pt (epoch 156 @ 0 updates)
| loading train data for epoch 156
| /scratch/project_2003453/bertnmt_data train fi-sv 30659795 examples
| saved checkpoint /scratch/project_2003453/bertnmt_data/bert-nmt_fi_sv/checkpoint156.pt (epoch 156 @ 0 updates) (writing took 1.0971920490264893 seconds)
| distributed init (rank 0): tcp://localhost:16510
| distributed init (rank 1): tcp://localhost:16510
| distributed init (rank 3): tcp://localhost:16510
| distributed init (rank 2): tcp://localhost:16510
| initialized host r03g02.bullx as rank 2
| initialized host r03g02.bullx as rank 0
| initialized host r03g02.bullx as rank 3
| initialized host r03g02.bullx as rank 1
Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, arch='transformer', attention_dropout=0.0, bert_first=True, bert_gates=[1, 1, 1, 1, 1, 1], bert_model_name='/scratch/project_2003453/bert-base-finnish-uncased-v1', bert_output_layer=-1, bert_ratio=1.0, bucket_cap_mb=25, clip_norm=25, cpu=False, criterion='label_smoothed_cross_entropy', curriculum=0, data='/scratch/project_2003453/bertnmt_data', dataset_impl='cached', ddp_backend='c10d', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layers=6, decoder_learned_pos=False, decoder_no_bert=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method='tcp://localhost:16510', distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=4, dropout=0.3, encoder_attention_heads=8, encoder_bert_dropout=True, encoder_bert_dropout_ratio=0.5, encoder_bert_mixup=False, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_learned_pos=False, encoder_normalize_before=False, encoder_ratio=1.0, find_unused_parameters=False, finetune_bert=False, fix_batches_to_gpus=False, fp16=True, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, lazy_load=False, left_pad_source='True', left_pad_target='False', log_format=None, log_interval=1000, lr=[0.0005], lr_scheduler='inverse_sqrt', mask_cls_sep=False, max_epoch=0, max_sentences=None, max_sentences_valid=None, max_source_positions=1024, max_target_positions=1024, max_tokens=16000, max_update=150000, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=1e-09, no_epoch_checkpoints=False, no_progress_bar=False, no_save=False, no_token_positional_embeddings=False, num_workers=0, optimizer='adam', optimizer_overrides='{}', raw_text=False, required_batch_size_multiple=8, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='/scratch/project_2003453/bertnmt_data/bert-nmt_fi_sv', save_interval=1, save_interval_updates=0, seed=1, sentence_avg=False, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, source_lang='fi', target_lang='sv', task='translation', tbmf_wrapper=False, tensorboard_logdir='', threshold_loss_scale=None, train_subset='train', update_freq=[1], upsample_primary=1, user_dir=None, valid_subset='valid', validate_interval=1, warmup_from_nmt=False, warmup_init_lr=1e-07, warmup_nmt_file='checkpoint_nmt.pt', warmup_updates=4000, weight_decay=0.0001)
| [fi] dictionary: 37752 types
| [sv] dictionary: 37752 types
| /scratch/project_2003453/bertnmt_data valid fi-sv 5000 examples
bert_gates [True, True, True, True, True, True]
TransformerModel(
  (encoder): TransformerEncoder(
    (embed_tokens): Embedding(37752, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (embed_tokens): Embedding(37752, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (bert_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (bert_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (bert_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (bert_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (bert_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (bert_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (bert_encoder): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(50101, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
)
| model transformer, criterion LabelSmoothedCrossEntropyCriterion
| num. model params: 195863040 (num. trained: 71344128)
| training on 4 GPUs
| max tokens per GPU = 16000 and max sentences per GPU = None
| loaded checkpoint /scratch/project_2003453/bertnmt_data/bert-nmt_fi_sv/checkpoint_last.pt (epoch 156 @ 0 updates)
| loading train data for epoch 156
| /scratch/project_2003453/bertnmt_data train fi-sv 30659795 examples
| distributed init (rank 2): tcp://localhost:16093
| distributed init (rank 3): tcp://localhost:16093
| distributed init (rank 0): tcp://localhost:16093
| distributed init (rank 1): tcp://localhost:16093
| initialized host r13g01.bullx as rank 3
| initialized host r13g01.bullx as rank 0
| initialized host r13g01.bullx as rank 2
| initialized host r13g01.bullx as rank 1
Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, arch='transformer', attention_dropout=0.0, bert_first=True, bert_gates=[1, 1, 1, 1, 1, 1], bert_model_name='/scratch/project_2003453/bert-base-finnish-uncased-v1', bert_output_layer=-1, bert_ratio=1.0, bucket_cap_mb=25, clip_norm=25, cpu=False, criterion='label_smoothed_cross_entropy', curriculum=0, data='/scratch/project_2003453/bertnmt_data', dataset_impl='cached', ddp_backend='c10d', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layers=6, decoder_learned_pos=False, decoder_no_bert=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method='tcp://localhost:16093', distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=4, dropout=0.3, encoder_attention_heads=8, encoder_bert_dropout=True, encoder_bert_dropout_ratio=0.5, encoder_bert_mixup=False, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_learned_pos=False, encoder_normalize_before=False, encoder_ratio=1.0, find_unused_parameters=False, finetune_bert=False, fix_batches_to_gpus=False, fp16=True, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, lazy_load=False, left_pad_source='True', left_pad_target='False', log_format=None, log_interval=1000, lr=[0.0005], lr_scheduler='inverse_sqrt', mask_cls_sep=False, max_epoch=0, max_sentences=None, max_sentences_valid=None, max_source_positions=1024, max_target_positions=1024, max_tokens=16000, max_update=150000, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=1e-09, no_epoch_checkpoints=False, no_progress_bar=False, no_save=False, no_token_positional_embeddings=False, num_workers=0, optimizer='adam', optimizer_overrides='{}', raw_text=False, required_batch_size_multiple=8, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='/scratch/project_2003453/bertnmt_data/bert-nmt_fi_sv', save_interval=1, save_interval_updates=0, seed=1, sentence_avg=False, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, source_lang='fi', target_lang='sv', task='translation', tbmf_wrapper=False, tensorboard_logdir='', threshold_loss_scale=None, train_subset='train', update_freq=[1], upsample_primary=1, user_dir=None, valid_subset='valid', validate_interval=1, warmup_from_nmt=False, warmup_init_lr=1e-07, warmup_nmt_file='checkpoint_nmt.pt', warmup_updates=4000, weight_decay=0.0001)
| [fi] dictionary: 37752 types
| [sv] dictionary: 37752 types
| /scratch/project_2003453/bertnmt_data valid fi-sv 5000 examples
bert_gates [True, True, True, True, True, True]
TransformerModel(
  (encoder): TransformerEncoder(
    (embed_tokens): Embedding(37752, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (embed_tokens): Embedding(37752, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (bert_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (bert_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (bert_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (bert_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (bert_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (bert_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (bert_encoder): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(50101, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
)
| model transformer, criterion LabelSmoothedCrossEntropyCriterion
| num. model params: 195863040 (num. trained: 71344128)
| training on 4 GPUs
| max tokens per GPU = 16000 and max sentences per GPU = None
| loaded checkpoint /scratch/project_2003453/bertnmt_data/bert-nmt_fi_sv/checkpoint_last.pt (epoch 156 @ 0 updates)
| loading train data for epoch 156
| /scratch/project_2003453/bertnmt_data train fi-sv 30659795 examples
| distributed init (rank 1): tcp://localhost:10563
| distributed init (rank 3): tcp://localhost:10563
| distributed init (rank 2): tcp://localhost:10563
| distributed init (rank 0): tcp://localhost:10563
| initialized host r13g01.bullx as rank 2
| initialized host r13g01.bullx as rank 1
| initialized host r13g01.bullx as rank 0
| initialized host r13g01.bullx as rank 3
Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, arch='transformer', attention_dropout=0.0, bert_first=True, bert_gates=[1, 1, 1, 1, 1, 1], bert_model_name='/scratch/project_2003453/bert-base-finnish-uncased-v1', bert_output_layer=-1, bert_ratio=1.0, bucket_cap_mb=25, clip_norm=25, cpu=False, criterion='label_smoothed_cross_entropy', curriculum=0, data='/scratch/project_2003453/bertnmt_data', dataset_impl='cached', ddp_backend='c10d', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layers=6, decoder_learned_pos=False, decoder_no_bert=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method='tcp://localhost:10563', distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=4, dropout=0.3, encoder_attention_heads=8, encoder_bert_dropout=True, encoder_bert_dropout_ratio=0.5, encoder_bert_mixup=False, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_learned_pos=False, encoder_normalize_before=False, encoder_ratio=1.0, find_unused_parameters=False, finetune_bert=False, fix_batches_to_gpus=False, fp16=True, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, lazy_load=False, left_pad_source='True', left_pad_target='False', log_format=None, log_interval=1000, lr=[0.0005], lr_scheduler='inverse_sqrt', mask_cls_sep=False, max_epoch=0, max_sentences=None, max_sentences_valid=None, max_source_positions=1024, max_target_positions=1024, max_tokens=16000, max_update=150000, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=1e-09, no_epoch_checkpoints=False, no_progress_bar=False, no_save=False, no_token_positional_embeddings=False, num_workers=0, optimizer='adam', optimizer_overrides='{}', raw_text=False, required_batch_size_multiple=8, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='/scratch/project_2003453/bertnmt_data/bert-nmt_fi_sv', save_interval=1, save_interval_updates=0, seed=1, sentence_avg=False, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, source_lang='fi', target_lang='sv', task='translation', tbmf_wrapper=False, tensorboard_logdir='', threshold_loss_scale=None, train_subset='train', update_freq=[1], upsample_primary=1, user_dir=None, valid_subset='valid', validate_interval=1, warmup_from_nmt=False, warmup_init_lr=1e-07, warmup_nmt_file='checkpoint_nmt.pt', warmup_updates=4000, weight_decay=0.0001)
| [fi] dictionary: 37752 types
| [sv] dictionary: 37752 types
| /scratch/project_2003453/bertnmt_data valid fi-sv 5000 examples
bert_gates [True, True, True, True, True, True]
TransformerModel(
  (encoder): TransformerEncoder(
    (embed_tokens): Embedding(37752, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (embed_tokens): Embedding(37752, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (bert_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (bert_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (bert_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (bert_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (bert_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (bert_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (bert_encoder): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(50101, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
)
| model transformer, criterion LabelSmoothedCrossEntropyCriterion
| num. model params: 195863040 (num. trained: 71344128)
| training on 4 GPUs
| max tokens per GPU = 16000 and max sentences per GPU = None
| loaded checkpoint /scratch/project_2003453/bertnmt_data/bert-nmt_fi_sv/checkpoint_last.pt (epoch 156 @ 0 updates)
| loading train data for epoch 156
| /scratch/project_2003453/bertnmt_data train fi-sv 30659795 examples
| distributed init (rank 2): tcp://localhost:18367
| distributed init (rank 0): tcp://localhost:18367
| distributed init (rank 3): tcp://localhost:18367
| distributed init (rank 1): tcp://localhost:18367
| initialized host r13g01.bullx as rank 0
| initialized host r13g01.bullx as rank 2
| initialized host r13g01.bullx as rank 3
| initialized host r13g01.bullx as rank 1
Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, arch='transformer', attention_dropout=0.0, bert_first=True, bert_gates=[1, 1, 1, 1, 1, 1], bert_model_name='/scratch/project_2003453/bert-base-finnish-uncased-v1', bert_output_layer=-1, bert_ratio=1.0, bucket_cap_mb=25, clip_norm=25, cpu=False, criterion='label_smoothed_cross_entropy', curriculum=0, data='/scratch/project_2003453/bertnmt_data', dataset_impl='cached', ddp_backend='c10d', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layers=6, decoder_learned_pos=False, decoder_no_bert=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method='tcp://localhost:18367', distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=4, dropout=0.3, encoder_attention_heads=8, encoder_bert_dropout=True, encoder_bert_dropout_ratio=0.5, encoder_bert_mixup=False, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_learned_pos=False, encoder_normalize_before=False, encoder_ratio=1.0, find_unused_parameters=False, finetune_bert=False, fix_batches_to_gpus=False, fp16=True, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, lazy_load=False, left_pad_source='True', left_pad_target='False', log_format=None, log_interval=1000, lr=[0.0005], lr_scheduler='inverse_sqrt', mask_cls_sep=False, max_epoch=0, max_sentences=None, max_sentences_valid=None, max_source_positions=1024, max_target_positions=1024, max_tokens=16000, max_update=150000, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=1e-09, no_epoch_checkpoints=False, no_progress_bar=False, no_save=False, no_token_positional_embeddings=False, num_workers=0, optimizer='adam', optimizer_overrides='{}', raw_text=False, required_batch_size_multiple=8, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='/scratch/project_2003453/bertnmt_data/bert-nmt_fi_sv', save_interval=1, save_interval_updates=0, seed=1, sentence_avg=False, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, source_lang='fi', target_lang='sv', task='translation', tbmf_wrapper=False, tensorboard_logdir='', threshold_loss_scale=None, train_subset='train', update_freq=[1], upsample_primary=1, user_dir=None, valid_subset='valid', validate_interval=1, warmup_from_nmt=False, warmup_init_lr=1e-07, warmup_nmt_file='checkpoint_nmt.pt', warmup_updates=4000, weight_decay=0.0001)
| [fi] dictionary: 37752 types
| [sv] dictionary: 37752 types
| /scratch/project_2003453/bertnmt_data valid fi-sv 5000 examples
bert_gates [True, True, True, True, True, True]
TransformerModel(
  (encoder): TransformerEncoder(
    (embed_tokens): Embedding(37752, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (embed_tokens): Embedding(37752, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (bert_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (bert_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (bert_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (bert_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (bert_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (bert_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (bert_encoder): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(50101, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
)| distributed init (rank 1): tcp://localhost:14803
| distributed init (rank 3): tcp://localhost:14803
| distributed init (rank 0): tcp://localhost:14803
| distributed init (rank 2): tcp://localhost:14803
| initialized host r03g06.bullx as rank 0
| initialized host r03g06.bullx as rank 1
| initialized host r03g06.bullx as rank 3
| initialized host r03g06.bullx as rank 2
Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, arch='transformer', attention_dropout=0.0, bert_first=True, bert_gates=[1, 1, 1, 1, 1, 1], bert_model_name='/scratch/project_2003453/bert-base-finnish-uncased-v1', bert_output_layer=-1, bert_ratio=1.0, bucket_cap_mb=25, clip_norm=25, cpu=False, criterion='label_smoothed_cross_entropy', curriculum=0, data='/scratch/project_2003453/bertnmt_data', dataset_impl='cached', ddp_backend='c10d', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layers=6, decoder_learned_pos=False, decoder_no_bert=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method='tcp://localhost:14803', distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=4, dropout=0.3, encoder_attention_heads=8, encoder_bert_dropout=True, encoder_bert_dropout_ratio=0.5, encoder_bert_mixup=False, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_learned_pos=False, encoder_normalize_before=False, encoder_ratio=1.0, find_unused_parameters=False, finetune_bert=False, fix_batches_to_gpus=False, fp16=True, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, lazy_load=False, left_pad_source='True', left_pad_target='False', log_format=None, log_interval=1000, lr=[0.0005], lr_scheduler='inverse_sqrt', mask_cls_sep=False, max_epoch=0, max_sentences=None, max_sentences_valid=None, max_source_positions=1024, max_target_positions=1024, max_tokens=16000, max_update=150000, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=1e-09, no_epoch_checkpoints=False, no_progress_bar=False, no_save=False, no_token_positional_embeddings=False, num_workers=0, optimizer='adam', optimizer_overrides='{}', raw_text=False, required_batch_size_multiple=8, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='/scratch/project_2003453/bertnmt_data/bert-nmt_fi_sv', save_interval=1, save_interval_updates=0, seed=1, sentence_avg=False, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, source_lang='fi', target_lang='sv', task='translation', tbmf_wrapper=False, tensorboard_logdir='', threshold_loss_scale=None, train_subset='train', update_freq=[1], upsample_primary=1, user_dir=None, valid_subset='valid', validate_interval=1, warmup_from_nmt=False, warmup_init_lr=1e-07, warmup_nmt_file='checkpoint_nmt.pt', warmup_updates=4000, weight_decay=0.0001)
| [fi] dictionary: 37752 types
| [sv] dictionary: 37752 types
| /scratch/project_2003453/bertnmt_data valid fi-sv 5000 examples
bert_gates [True, True, True, True, True, True]
TransformerModel(
  (encoder): TransformerEncoder(
    (embed_tokens): Embedding(37752, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (embed_tokens): Embedding(37752, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (bert_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (bert_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (bert_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (bert_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (bert_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (bert_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (bert_encoder): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(50101, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
)
| model transformer, criterion LabelSmoothedCrossEntropyCriterion
| num. model params: 195863040 (num. trained: 71344128)
| training on 4 GPUs
| max tokens per GPU = 16000 and max sentences per GPU = None
| loaded checkpoint /scratch/project_2003453/bertnmt_data/bert-nmt_fi_sv/checkpoint_last.pt (epoch 156 @ 0 updates)
| loading train data for epoch 156
| /scratch/project_2003453/bertnmt_data train fi-sv 30659795 examples
| WARNING: overflow detected, setting loss scale to: 64.0
| WARNING: overflow detected, setting loss scale to: 32.0
| WARNING: overflow detected, setting loss scale to: 16.0
| WARNING: overflow detected, setting loss scale to: 8.0
| WARNING: overflow detected, setting loss scale to: 4.0
| WARNING: overflow detected, setting loss scale to: 2.0
| WARNING: overflow detected, setting loss scale to: 1.0
| WARNING: overflow detected, setting loss scale to: 0.5
| WARNING: overflow detected, setting loss scale to: 0.25
Invalid bert_input shape:torch.Size([16, 760])
| epoch 156:   9000 / 11953 loss=8.311, nll_loss=7.199, ppl=146.96, wps=39123, ups=1, wpb=41532.002, bsz=2511.484, num_updates=465, lr=5.82134e-05, gnorm=4.837, clip=0.002, oom=0.000, loss_scale=0.250, wall=494, train_wall=414281
Invalid bert_input shape:torch.Size([16, 729])
Invalid bert_input shape:torch.Size([16, 831])
Invalid bert_input shape:torch.Size([24, 575])
| epoch 156:  10000 / 11953 loss=7.011, nll_loss=5.703, ppl=52.09, wps=67758, ups=2, wpb=41660.716, bsz=2550.678, num_updates=1465, lr=0.000183188, gnorm=3.845, clip=0.001, oom=0.000, loss_scale=0.250, wall=901, train_wall=414609
Invalid bert_input shape:torch.Size([16, 722])
Invalid bert_input shape:torch.Size([16, 817])
| epoch 156:  11000 / 11953 loss=6.584, nll_loss=5.212, ppl=37.06, wps=78762, ups=2, wpb=41588.869, bsz=2552.750, num_updates=2465, lr=0.000308163, gnorm=3.457, clip=0.001, oom=0.000, loss_scale=0.250, wall=1302, train_wall=414932
| epoch 156 | loss 6.383 | nll_loss 4.981 | ppl 31.58 | wps 84083 | ups 2 | wpb 41608.200 | bsz 2560.714 | num_updates 3417 | lr 0.00042714 | gnorm 3.191 | clip 0.001 | oom 0.000 | loss_scale 0.250 | wall 1691 | train_wall 415247
| epoch 156 | valid on 'valid' subset | loss 3.673 | nll_loss 1.792 | ppl 3.46 | num_updates 3417 | best_loss 3.19798
| saved checkpoint /scratch/project_2003453/bertnmt_data/bert-nmt_fi_sv/checkpoint156.pt (epoch 156 @ 3417 updates) (writing took 5.280877113342285 seconds)
| epoch 157:   1000 / 11953 loss=5.815, nll_loss=4.332, ppl=20.14, wps=103098, ups=2, wpb=41782.649, bsz=2533.789, num_updates=4418, lr=0.000475759, gnorm=1.957, clip=0.000, oom=0.000, loss_scale=0.500, wall=2237, train_wall=415586
Invalid bert_input shape:torch.Size([8, 1203])
| epoch 157:   2000 / 11953 loss=5.734, nll_loss=4.244, ppl=18.94, wps=102870, ups=2, wpb=41787.183, bsz=2530.420, num_updates=5418, lr=0.000429616, gnorm=1.667, clip=0.000, oom=0.000, loss_scale=0.500, wall=2644, train_wall=415919
Invalid bert_input shape:torch.Size([16, 776])
| epoch 157:   3000 / 11953 loss=5.659, nll_loss=4.161, ppl=17.89, wps=101984, ups=2, wpb=41589.435, bsz=2551.244, num_updates=6418, lr=0.00039473, gnorm=1.517, clip=0.000, oom=0.000, loss_scale=0.500, wall=3055, train_wall=416252
| epoch 157:   4000 / 11953 loss=5.605, nll_loss=4.101, ppl=17.16, wps=102753, ups=2, wpb=41655.009, bsz=2549.701, num_updates=7418, lr=0.000367161, gnorm=1.420, clip=0.000, oom=0.000, loss_scale=0.500, wall=3454, train_wall=416572
| epoch 157:   5000 / 11953 loss=5.558, nll_loss=4.049, ppl=16.55, wps=103238, ups=2, wpb=41759.886, bsz=2547.050, num_updates=8418, lr=0.000344664, gnorm=1.349, clip=0.000, oom=0.000, loss_scale=1.000, wall=3854, train_wall=416898
Invalid bert_input shape:torch.Size([24, 602])
Invalid bert_input shape:torch.Size([16, 869])
Invalid bert_input shape:torch.Size([8, 1160])
| epoch 157:   6000 / 11953 loss=5.519, nll_loss=4.006, ppl=16.06, wps=103146, ups=2, wpb=41750.455, bsz=2554.707, num_updates=9418, lr=0.000325852, gnorm=1.306, clip=0.000, oom=0.000, loss_scale=1.000, wall=4261, train_wall=417228
| epoch 157:   7000 / 11953 loss=5.489, nll_loss=3.972, ppl=15.69, wps=103258, ups=2, wpb=41804.420, bsz=2552.566, num_updates=10418, lr=0.000309819, gnorm=1.264, clip=0.000, oom=0.000, loss_scale=1.000, wall=4666, train_wall=417550
Invalid bert_input shape:torch.Size([24, 547])
Invalid bert_input shape:torch.Size([8, 1608])
Invalid bert_input shape:torch.Size([16, 701])
Invalid bert_input shape:torch.Size([16, 859])
| epoch 157:   8000 / 11953 loss=5.457, nll_loss=3.937, ppl=15.31, wps=103290, ups=2, wpb=41849.879, bsz=2557.368, num_updates=11418, lr=0.000295941, gnorm=1.230, clip=0.000, oom=0.000, loss_scale=1.000, wall=5073, train_wall=417877
| epoch 157:   9000 / 11953 loss=5.430, nll_loss=3.906, ppl=15.00, wps=103382, ups=2, wpb=41882.227, bsz=2564.538, num_updates=12418, lr=0.000283775, gnorm=1.201, clip=0.000, oom=0.000, loss_scale=2.000, wall=5478, train_wall=418206
Invalid bert_input shape:torch.Size([8, 1334])
Invalid bert_input shape:torch.Size([24, 575])
| epoch 157:  10000 / 11953 loss=5.406, nll_loss=3.879, ppl=14.71, wps=103318, ups=2, wpb=41904.744, bsz=2567.737, num_updates=13418, lr=0.000272996, gnorm=1.175, clip=0.000, oom=0.000, loss_scale=2.000, wall=5888, train_wall=418543
Invalid bert_input shape:torch.Size([16, 729])
| WARNING: overflow detected, setting loss scale to: 1.0
Invalid bert_input shape:torch.Size([16, 832])
| epoch 157:  11000 / 11953 loss=5.384, nll_loss=3.855, ppl=14.47, wps=103170, ups=2, wpb=41900.984, bsz=2565.564, num_updates=14417, lr=0.000263368, gnorm=1.154, clip=0.000, oom=0.000, loss_scale=1.000, wall=6299, train_wall=418863
| epoch 157 | loss 5.365 | nll_loss 3.834 | ppl 14.26 | wps 103033 | ups 2 | wpb 41864.568 | bsz 2564.943 | num_updates 15369 | lr 0.00025508 | gnorm 1.135 | clip 0.000 | oom 0.000 | loss_scale 1.000 | wall 6688 | train_wall 419166
| epoch 157 | valid on 'valid' subset | loss 3.388 | nll_loss 1.536 | ppl 2.90 | num_updates 15369 | best_loss 3.19798
| saved checkpoint /scratch/project_2003453/bertnmt_data/bert-nmt_fi_sv/checkpoint157.pt (epoch 157 @ 15369 updates) (writing took 3.848905086517334 seconds)
Invalid bert_input shape:torch.Size([24, 550])
| epoch 158:   1000 / 11953 loss=5.108, nll_loss=3.547, ppl=11.69, wps=103150, ups=2, wpb=41948.396, bsz=2531.820, num_updates=16370, lr=0.000247159, gnorm=0.899, clip=0.000, oom=0.000, loss_scale=1.000, wall=7231, train_wall=419497
Invalid bert_input shape:torch.Size([16, 907])
Invalid bert_input shape:torch.Size([8, 1184])
Invalid bert_input shape:torch.Size([8, 1356])
| epoch 158:   2000 / 11953 loss=5.100, nll_loss=3.538, ppl=11.62, wps=103581, ups=2, wpb=41767.243, bsz=2562.999, num_updates=17370, lr=0.000239939, gnorm=0.905, clip=0.000, oom=0.000, loss_scale=1.000, wall=7631, train_wall=419821
Invalid bert_input shape:torch.Size([24, 627])
| WARNING: overflow detected, setting loss scale to: 1.0
| epoch 158:   3000 / 11953 loss=5.096, nll_loss=3.534, ppl=11.59, wps=102864, ups=2, wpb=41749.952, bsz=2554.371, num_updates=18369, lr=0.000233323, gnorm=0.902, clip=0.000, oom=0.000, loss_scale=1.000, wall=8042, train_wall=420132
| epoch 158:   4000 / 11953 loss=5.090, nll_loss=3.528, ppl=11.54, wps=102954, ups=2, wpb=41727.212, bsz=2563.988, num_updates=19369, lr=0.00022722, gnorm=0.901, clip=0.000, oom=0.000, loss_scale=1.000, wall=8445, train_wall=420460
Invalid bert_input shape:torch.Size([16, 790])
Invalid bert_input shape:torch.Size([16, 704])
Invalid bert_input shape:torch.Size([8, 1608])
Invalid bert_input shape:torch.Size([16, 760])
| epoch 158:   5000 / 11953 loss=5.085, nll_loss=3.522, ppl=11.49, wps=102837, ups=2, wpb=41799.503, bsz=2559.147, num_updates=20369, lr=0.000221572, gnorm=0.895, clip=0.000, oom=0.000, loss_scale=1.000, wall=8856, train_wall=420793
Invalid bert_input shape:torch.Size([8, 1119])
| epoch 158:   6000 / 11953 loss=5.079, nll_loss=3.516, ppl=11.44, wps=102997, ups=2, wpb=41822.616, bsz=2565.691, num_updates=21369, lr=0.000216326, gnorm=0.894, clip=0.000, oom=0.000, loss_scale=1.000, wall=9260, train_wall=421118
Invalid bert_input shape:torch.Size([16, 900])
Invalid bert_input shape:torch.Size([24, 614])
Invalid bert_input shape:torch.Size([8, 1203])
| epoch 158:   7000 / 11953 loss=5.068, nll_loss=3.504, ppl=11.34, wps=103199, ups=2, wpb=41831.693, bsz=2572.671, num_updates=22369, lr=0.000211435, gnorm=0.887, clip=0.000, oom=0.000, loss_scale=2.000, wall=9661, train_wall=421440
| epoch 158:   8000 / 11953 loss=5.061, nll_loss=3.495, ppl=11.28, wps=103345, ups=2, wpb=41855.357, bsz=2564.466, num_updates=23369, lr=0.000206862, gnorm=0.881, clip=0.000, oom=0.000, loss_scale=2.000, wall=10064, train_wall=421767
| WARNING: overflow detected, setting loss scale to: 1.0
Invalid bert_input shape:torch.Size([24, 656])
| epoch 158:   9000 / 11953 loss=5.056, nll_loss=3.490, ppl=11.23, wps=103527, ups=2, wpb=41902.829, bsz=2561.820, num_updates=24368, lr=0.000202577, gnorm=0.878, clip=0.000, oom=0.000, loss_scale=1.000, wall=10466, train_wall=422094
Invalid bert_input shape:torch.Size([24, 638])
| epoch 158:  10000 / 11953 loss=5.051, nll_loss=3.484, ppl=11.19, wps=103190, ups=2, wpb=41866.758, bsz=2561.337, num_updates=25368, lr=0.000198544, gnorm=0.877, clip=0.000, oom=0.000, loss_scale=1.000, wall=10881, train_wall=422431
Invalid bert_input shape:torch.Size([16, 869])
| WARNING: overflow detected, setting loss scale to: 0.5
Invalid bert_input shape:torch.Size([16, 837])
| epoch 158:  11000 / 11953 loss=5.044, nll_loss=3.476, ppl=11.13, wps=102853, ups=2, wpb=41827.962, bsz=2563.602, num_updates=26367, lr=0.000194746, gnorm=0.875, clip=0.000, oom=0.000, loss_scale=0.500, wall=11297, train_wall=422752
| epoch 158 | loss 5.038 | nll_loss 3.470 | ppl 11.08 | wps 102917 | ups 2 | wpb 41863.405 | bsz 2564.499 | num_updates 27319 | lr 0.000191323 | gnorm 0.873 | clip 0.000 | oom 0.000 | loss_scale 0.500 | wall 11685 | train_wall 423066
| epoch 158 | valid on 'valid' subset | loss 3.319 | nll_loss 1.467 | ppl 2.76 | num_updates 27319 | best_loss 3.19798
| saved checkpoint /scratch/project_2003453/bertnmt_data/bert-nmt_fi_sv/checkpoint158.pt (epoch 158 @ 27319 updates) (writing took 4.075748682022095 seconds)
Invalid bert_input shape:torch.Size([8, 1160])
Invalid bert_input shape:torch.Size([16, 784])
| epoch 159:   1000 / 11953 loss=4.970, nll_loss=3.395, ppl=10.52, wps=103753, ups=2, wpb=42125.169, bsz=2521.023, num_updates=28320, lr=0.000187912, gnorm=0.840, clip=0.000, oom=0.000, loss_scale=0.500, wall=12232, train_wall=423405
Invalid bert_input shape:torch.Size([16, 844])
| epoch 159:   2000 / 11953 loss=4.963, nll_loss=3.387, ppl=10.46, wps=103775, ups=2, wpb=42143.487, bsz=2562.867, num_updates=29320, lr=0.000184679, gnorm=0.834, clip=0.000, oom=0.000, loss_scale=0.500, wall=12639, train_wall=423730
Invalid bert_input shape:torch.Size([24, 589])
Invalid bert_input shape:torch.Size([16, 823])
| epoch 159:   3000 / 11953 loss=4.949, nll_loss=3.372, ppl=10.35, wps=103490, ups=2, wpb=41970.748, bsz=2576.701, num_updates=30320, lr=0.000181608, gnorm=0.829, clip=0.000, oom=0.000, loss_scale=1.000, wall=13043, train_wall=424054
Invalid bert_input shape:torch.Size([16, 869])
| epoch 159:   4000 / 11953 loss=4.948, nll_loss=3.371, ppl=10.34, wps=103404, ups=2, wpb=41834.267, bsz=2566.682, num_updates=31320, lr=0.000178685, gnorm=0.828, clip=0.000, oom=0.000, loss_scale=1.000, wall=13445, train_wall=424376
Invalid bert_input shape:torch.Size([16, 795])
Invalid bert_input shape:torch.Size([16, 944])
| epoch 159:   5000 / 11953 loss=4.944, nll_loss=3.366, ppl=10.31, wps=103669, ups=2, wpb=41933.379, bsz=2565.882, num_updates=32320, lr=0.000175899, gnorm=0.826, clip=0.000, oom=0.000, loss_scale=1.000, wall=13849, train_wall=424707
Invalid bert_input shape:torch.Size([16, 859])
| epoch 159:   6000 / 11953 loss=4.940, nll_loss=3.361, ppl=10.28, wps=103753, ups=2, wpb=41916.000, bsz=2565.090, num_updates=33320, lr=0.00017324, gnorm=0.827, clip=0.000, oom=0.000, loss_scale=1.000, wall=14250, train_wall=425031
| epoch 159:   7000 / 11953 loss=4.935, nll_loss=3.356, ppl=10.24, wps=103442, ups=2, wpb=41901.098, bsz=2563.222, num_updates=34320, lr=0.000170697, gnorm=0.826, clip=0.000, oom=0.000, loss_scale=2.000, wall=14662, train_wall=425366
Invalid bert_input shape:torch.Size([16, 994])
| WARNING: overflow detected, setting loss scale to: 1.0
Invalid bert_input shape:torch.Size([16, 729])
| epoch 159:   8000 / 11953 loss=4.932, nll_loss=3.352, ppl=10.21, wps=103104, ups=2, wpb=41857.539, bsz=2566.232, num_updates=35319, lr=0.000168266, gnorm=0.829, clip=0.000, oom=0.000, loss_scale=1.000, wall=15074, train_wall=425700
Invalid bert_input shape:torch.Size([16, 790])
Invalid bert_input shape:torch.Size([16, 721])
| epoch 159:   9000 / 11953 loss=4.927, nll_loss=3.347, ppl=10.17, wps=103212, ups=2, wpb=41876.311, bsz=2570.502, num_updates=36319, lr=0.000165933, gnorm=0.827, clip=0.000, oom=0.000, loss_scale=1.000, wall=15477, train_wall=426024
Invalid bert_input shape:torch.Size([16, 776])
Invalid bert_input shape:torch.Size([16, 832])
| epoch 159:  10000 / 11953 loss=4.922, nll_loss=3.342, ppl=10.14, wps=103225, ups=2, wpb=41884.577, bsz=2569.559, num_updates=37319, lr=0.000163695, gnorm=0.825, clip=0.000, oom=0.000, loss_scale=1.000, wall=15884, train_wall=426348
Invalid bert_input shape:torch.Size([24, 600])
| epoch 159:  11000 / 11953 loss=4.920, nll_loss=3.340, ppl=10.12, wps=103175, ups=2, wpb=41893.130, bsz=2564.896, num_updates=38319, lr=0.000161545, gnorm=0.824, clip=0.000, oom=0.000, loss_scale=1.000, wall=16292, train_wall=426682
Invalid bert_input shape:torch.Size([24, 575])
Invalid bert_input shape:torch.Size([8, 1356])
| epoch 159 | loss 4.917 | nll_loss 3.336 | ppl 10.10 | wps 103117 | ups 2 | wpb 41864.194 | bsz 2565.038 | num_updates 39271 | lr 0.000159575 | gnorm 0.823 | clip 0.000 | oom 0.000 | loss_scale 2.000 | wall 16678 | train_wall 426993
| epoch 159 | valid on 'valid' subset | loss 3.262 | nll_loss 1.412 | ppl 2.66 | num_updates 39271 | best_loss 3.19798
| saved checkpoint /scratch/project_2003453/bertnmt_data/bert-nmt_fi_sv/checkpoint159.pt (epoch 159 @ 39271 updates) (writing took 5.164485454559326 seconds)
Invalid bert_input shape:torch.Size([16, 843])
| WARNING: overflow detected, setting loss scale to: 1.0
Invalid bert_input shape:torch.Size([24, 600])
| epoch 160:   1000 / 11953 loss=4.876, nll_loss=3.291, ppl=9.79, wps=104041, ups=2, wpb=41825.350, bsz=2520.440, num_updates=40271, lr=0.000157581, gnorm=0.823, clip=0.000, oom=0.000, loss_scale=1.000, wall=17213, train_wall=427321
Invalid bert_input shape:torch.Size([24, 666])
Invalid bert_input shape:torch.Size([24, 638])
Invalid bert_input shape:torch.Size([16, 784])
| epoch 160:   2000 / 11953 loss=4.883, nll_loss=3.298, ppl=9.84, wps=103723, ups=2, wpb=41938.584, bsz=2524.252, num_updates=41271, lr=0.00015566, gnorm=0.812, clip=0.000, oom=0.000, loss_scale=1.000, wall=17620, train_wall=427644
Invalid bert_input shape:torch.Size([16, 832])
Invalid bert_input shape:torch.Size([16, 720])
| epoch 160:   3000 / 11953 loss=4.876, nll_loss=3.291, ppl=9.79, wps=103911, ups=2, wpb=41952.318, bsz=2543.397, num_updates=42271, lr=0.000153808, gnorm=0.813, clip=0.000, oom=0.000, loss_scale=1.000, wall=18022, train_wall=427967
Invalid bert_input shape:torch.Size([16, 950])
Invalid bert_input shape:torch.Size([24, 566])
| epoch 160:   4000 / 11953 loss=4.869, nll_loss=3.282, ppl=9.73, wps=103967, ups=2, wpb=41957.478, bsz=2543.158, num_updates=43271, lr=0.00015202, gnorm=0.811, clip=0.000, oom=0.000, loss_scale=1.000, wall=18425, train_wall=428294
Invalid bert_input shape:torch.Size([24, 602])
Invalid bert_input shape:torch.Size([8, 1562])
| epoch 160:   5000 / 11953 loss=4.863, nll_loss=3.276, ppl=9.69, wps=104023, ups=2, wpb=41919.173, bsz=2543.717, num_updates=44271, lr=0.000150294, gnorm=0.809, clip=0.000, oom=0.000, loss_scale=2.000, wall=18826, train_wall=428617
Invalid bert_input shape:torch.Size([16, 944])
| epoch 160:   6000 / 11953 loss=4.861, nll_loss=3.274, ppl=9.67, wps=103727, ups=2, wpb=41920.346, bsz=2543.752, num_updates=45271, lr=0.000148624, gnorm=0.809, clip=0.000, oom=0.000, loss_scale=2.000, wall=19236, train_wall=428948
| WARNING: overflow detected, setting loss scale to: 1.0
Invalid bert_input shape:torch.Size([16, 692])
| epoch 160:   7000 / 11953 loss=4.861, nll_loss=3.274, ppl=9.67, wps=103538, ups=2, wpb=41913.919, bsz=2545.407, num_updates=46270, lr=0.000147011, gnorm=0.808, clip=0.000, oom=0.000, loss_scale=1.000, wall=19644, train_wall=429279
Invalid bert_input shape:torch.Size([8, 1160])
Invalid bert_input shape:torch.Size([24, 595])
| epoch 160:   8000 / 11953 loss=4.858, nll_loss=3.271, ppl=9.65, wps=103298, ups=2, wpb=41900.019, bsz=2551.669, num_updates=47270, lr=0.000145448, gnorm=0.808, clip=0.000, oom=0.000, loss_scale=1.000, wall=20056, train_wall=429605
| epoch 160:   9000 / 11953 loss=4.854, nll_loss=3.266, ppl=9.62, wps=103141, ups=2, wpb=41889.599, bsz=2556.174, num_updates=48270, lr=0.000143933, gnorm=0.806, clip=0.000, oom=0.000, loss_scale=1.000, wall=20466, train_wall=429936
Invalid bert_input shape:torch.Size([16, 790])
Invalid bert_input shape:torch.Size([16, 701])
Invalid bert_input shape:torch.Size([8, 1513])
Invalid bert_input shape:torch.Size([16, 722])
| epoch 160:  10000 / 11953 loss=4.852, nll_loss=3.265, ppl=9.61, wps=103394, ups=2, wpb=41884.403, bsz=2557.971, num_updates=49270, lr=0.000142465, gnorm=0.807, clip=0.000, oom=0.000, loss_scale=1.000, wall=20862, train_wall=430258
| WARNING: overflow detected, setting loss scale to: 1.0
Invalid bert_input shape:torch.Size([16, 896])
| epoch 160:  11000 / 11953 loss=4.849, nll_loss=3.261, ppl=9.59, wps=103458, ups=2, wpb=41882.222, bsz=2561.425, num_updates=50269, lr=0.000141042, gnorm=0.806, clip=0.000, oom=0.000, loss_scale=1.000, wall=21263, train_wall=430582
Invalid bert_input shape:torch.Size([16, 864])
| epoch 160 | loss 4.845 | nll_loss 3.257 | ppl 9.56 | wps 103594 | ups 2 | wpb 41864.735 | bsz 2564.571 | num_updates 51221 | lr 0.000139726 | gnorm 0.805 | clip 0.000 | oom 0.000 | loss_scale 1.000 | wall 21640 | train_wall 430885
| epoch 160 | valid on 'valid' subset | loss 3.241 | nll_loss 1.398 | ppl 2.64 | num_updates 51221 | best_loss 3.19798
| saved checkpoint /scratch/project_2003453/bertnmt_data/bert-nmt_fi_sv/checkpoint160.pt (epoch 160 @ 51221 updates) (writing took 3.9888975620269775 seconds)
Invalid bert_input shape:torch.Size([16, 900])
| epoch 161:   1000 / 11953 loss=4.813, nll_loss=3.221, ppl=9.32, wps=105196, ups=2, wpb=41765.453, bsz=2546.094, num_updates=52222, lr=0.00013838, gnorm=0.807, clip=0.000, oom=0.000, loss_scale=1.000, wall=22167, train_wall=431209
| epoch 161:   2000 / 11953 loss=4.815, nll_loss=3.223, ppl=9.34, wps=105303, ups=2, wpb=41985.318, bsz=2543.356, num_updates=53222, lr=0.000137074, gnorm=0.801, clip=0.000, oom=0.000, loss_scale=1.000, wall=22568, train_wall=431532
| epoch 161:   3000 / 11953 loss=4.810, nll_loss=3.218, ppl=9.30, wps=104414, ups=2, wpb=41959.785, bsz=2555.804, num_updates=54222, lr=0.000135804, gnorm=0.800, clip=0.000, oom=0.000, loss_scale=2.000, wall=22976, train_wall=431865
Invalid bert_input shape:torch.Size([16, 704])
Invalid bert_input shape:torch.Size([24, 600])
Invalid bert_input shape:torch.Size([8, 1167])
| epoch 161:   4000 / 11953 loss=4.811, nll_loss=3.219, ppl=9.31, wps=104296, ups=2, wpb=41948.353, bsz=2562.988, num_updates=55222, lr=0.000134569, gnorm=0.799, clip=0.000, oom=0.000, loss_scale=2.000, wall=23379, train_wall=432185
Invalid bert_input shape:torch.Size([16, 701])
Invalid bert_input shape:torch.Size([8, 1562])
| epoch 161:   5000 / 11953 loss=4.806, nll_loss=3.214, ppl=9.28, wps=104444, ups=2, wpb=41942.651, bsz=2564.992, num_updates=56222, lr=0.000133367, gnorm=0.801, clip=0.000, oom=0.000, loss_scale=2.000, wall=23778, train_wall=432508
Invalid bert_input shape:torch.Size([16, 863])
Invalid bert_input shape:torch.Size([16, 720])
Invalid bert_input shape:torch.Size([16, 837])
| epoch 161:   6000 / 11953 loss=4.806, nll_loss=3.213, ppl=9.27, wps=103876, ups=2, wpb=41899.981, bsz=2567.070, num_updates=57222, lr=0.000132196, gnorm=0.803, clip=0.000, oom=0.000, loss_scale=2.000, wall=24191, train_wall=432821
| WARNING: overflow detected, setting loss scale to: 1.0
Invalid bert_input shape:torch.Size([16, 674])
Invalid bert_input shape:torch.Size([8, 1184])
| epoch 161:   7000 / 11953 loss=4.805, nll_loss=3.212, ppl=9.27, wps=103352, ups=2, wpb=41880.811, bsz=2561.278, num_updates=58221, lr=0.000131057, gnorm=0.804, clip=0.000, oom=0.000, loss_scale=1.000, wall=24606, train_wall=433128
| epoch 161:   8000 / 11953 loss=4.799, nll_loss=3.206, ppl=9.22, wps=103046, ups=2, wpb=41872.839, bsz=2568.315, num_updates=59221, lr=0.000129946, gnorm=0.801, clip=0.000, oom=0.000, loss_scale=1.000, wall=25021, train_wall=433452
Invalid bert_input shape:torch.Size([16, 950])
Invalid bert_input shape:torch.Size([8, 1356])
| epoch 161:   9000 / 11953 loss=4.797, nll_loss=3.204, ppl=9.21, wps=103177, ups=2, wpb=41875.568, bsz=2567.284, num_updates=60221, lr=0.000128862, gnorm=0.798, clip=0.000, oom=0.000, loss_scale=1.000, wall=25423, train_wall=433774
Invalid bert_input shape:torch.Size([8, 1119])
| epoch 161:  10000 / 11953 loss=4.796, nll_loss=3.203, ppl=9.21, wps=102783, ups=2, wpb=41849.977, bsz=2571.003, num_updates=61221, lr=0.000127806, gnorm=0.796, clip=0.000, oom=0.000, loss_scale=1.000, wall=25842, train_wall=434111
| epoch 161:  11000 / 11953 loss=4.795, nll_loss=3.201, ppl=9.20, wps=102996, ups=2, wpb=41840.635, bsz=2567.373, num_updates=62221, lr=0.000126774, gnorm=0.796, clip=0.000, oom=0.000, loss_scale=2.000, wall=26239, train_wall=434430
Invalid bert_input shape:torch.Size([24, 589])
| WARNING: overflow detected, setting loss scale to: 1.0
Invalid bert_input shape:torch.Size([16, 859])
| epoch 161 | loss 4.794 | nll_loss 3.201 | ppl 9.20 | wps 102855 | ups 2 | wpb 41863.443 | bsz 2564.803 | num_updates 63172 | lr 0.000125817 | gnorm 0.794 | clip 0.000 | oom 0.000 | loss_scale 1.000 | wall 26634 | train_wall 434728
| epoch 161 | valid on 'valid' subset | loss 3.227 | nll_loss 1.381 | ppl 2.60 | num_updates 63172 | best_loss 3.19798
| saved checkpoint /scratch/project_2003453/bertnmt_data/bert-nmt_fi_sv/checkpoint161.pt (epoch 161 @ 63172 updates) (writing took 3.305149793624878 seconds)
Invalid bert_input shape:torch.Size([16, 900])
| epoch 162:   1000 / 11953 loss=4.754, nll_loss=3.155, ppl=8.91, wps=102588, ups=2, wpb=42021.836, bsz=2554.637, num_updates=64173, lr=0.000124831, gnorm=0.786, clip=0.000, oom=0.000, loss_scale=1.000, wall=27170, train_wall=435061
Invalid bert_input shape:torch.Size([24, 573])
| epoch 162:   2000 / 11953 loss=4.766, nll_loss=3.169, ppl=9.00, wps=102894, ups=2, wpb=41844.607, bsz=2558.661, num_updates=65173, lr=0.00012387, gnorm=0.794, clip=0.000, oom=0.000, loss_scale=1.000, wall=27574, train_wall=435388
Invalid bert_input shape:torch.Size([16, 704])
Invalid bert_input shape:torch.Size([16, 720])
| epoch 162:   3000 / 11953 loss=4.765, nll_loss=3.168, ppl=8.99, wps=102788, ups=2, wpb=41845.135, bsz=2565.737, num_updates=66173, lr=0.00012293, gnorm=0.792, clip=0.000, oom=0.000, loss_scale=1.000, wall=27982, train_wall=435716
Invalid bert_input shape:torch.Size([8, 1203])
Invalid bert_input shape:torch.Size([24, 575])
| epoch 162:   4000 / 11953 loss=4.761, nll_loss=3.164, ppl=8.96, wps=103343, ups=2, wpb=41852.992, bsz=2561.262, num_updates=67173, lr=0.000122012, gnorm=0.792, clip=0.000, oom=0.000, loss_scale=2.000, wall=28381, train_wall=436039
| WARNING: overflow detected, setting loss scale to: 1.0
Invalid bert_input shape:torch.Size([16, 703])
| epoch 162:   5000 / 11953 loss=4.763, nll_loss=3.166, ppl=8.98, wps=103439, ups=2, wpb=41899.872, bsz=2560.117, num_updates=68172, lr=0.000121115, gnorm=0.789, clip=0.000, oom=0.000, loss_scale=1.000, wall=28786, train_wall=436358
Invalid bert_input shape:torch.Size([16, 944])
| epoch 162:   6000 / 11953 loss=4.765, nll_loss=3.169, ppl=8.99, wps=103664, ups=2, wpb=41883.961, bsz=2560.192, num_updates=69172, lr=0.000120236, gnorm=0.789, clip=0.000, oom=0.000, loss_scale=1.000, wall=29184, train_wall=436675
Invalid bert_input shape:torch.Size([8, 1562])
| epoch 162:   7000 / 11953 loss=4.763, nll_loss=3.166, ppl=8.98, wps=103595, ups=2, wpb=41899.195, bsz=2562.233, num_updates=70172, lr=0.000119376, gnorm=0.790, clip=0.000, oom=0.000, loss_scale=1.000, wall=29591, train_wall=436992
| epoch 162:   8000 / 11953 loss=4.762, nll_loss=3.165, ppl=8.97, wps=103474, ups=2, wpb=41876.621, bsz=2557.039, num_updates=71172, lr=0.000118535, gnorm=0.788, clip=0.000, oom=0.000, loss_scale=1.000, wall=29998, train_wall=437318
Invalid bert_input shape:torch.Size([24, 550])
| WARNING: overflow detected, setting loss scale to: 1.0
| epoch 162:   9000 / 11953 loss=4.759, nll_loss=3.162, ppl=8.95, wps=103444, ups=2, wpb=41873.520, bsz=2557.209, num_updates=72171, lr=0.000117711, gnorm=0.789, clip=0.000, oom=0.000, loss_scale=1.000, wall=30403, train_wall=437648
Invalid bert_input shape:torch.Size([16, 823])
| epoch 162:  10000 / 11953 loss=4.759, nll_loss=3.161, ppl=8.95, wps=103008, ups=2, wpb=41878.930, bsz=2560.734, num_updates=73171, lr=0.000116904, gnorm=0.790, clip=0.000, oom=0.000, loss_scale=1.000, wall=30825, train_wall=437990
Invalid bert_input shape:torch.Size([16, 840])
Invalid bert_input shape:torch.Size([16, 721])
| epoch 162:  11000 / 11953 loss=4.758, nll_loss=3.161, ppl=8.94, wps=102873, ups=2, wpb=41903.215, bsz=2564.945, num_updates=74171, lr=0.000116114, gnorm=0.789, clip=0.000, oom=0.000, loss_scale=1.000, wall=31240, train_wall=438329
Invalid bert_input shape:torch.Size([16, 758])
Invalid bert_input shape:torch.Size([24, 644])
Invalid bert_input shape:torch.Size([16, 864])
| epoch 162 | loss 4.756 | nll_loss 3.159 | ppl 8.93 | wps 102630 | ups 2 | wpb 41863.273 | bsz 2564.607 | num_updates 75123 | lr 0.000115375 | gnorm 0.789 | clip 0.000 | oom 0.000 | loss_scale 1.000 | wall 31635 | train_wall 438648
| epoch 162 | valid on 'valid' subset | loss 3.231 | nll_loss 1.400 | ppl 2.64 | num_updates 75123 | best_loss 3.19798
| saved checkpoint /scratch/project_2003453/bertnmt_data/bert-nmt_fi_sv/checkpoint162.pt (epoch 162 @ 75123 updates) (writing took 5.075678586959839 seconds)
Invalid bert_input shape:torch.Size([8, 1334])
Invalid bert_input shape:torch.Size([16, 844])
Invalid bert_input shape:torch.Size([8, 1203])
Invalid bert_input shape:torch.Size([16, 994])
| epoch 163:   1000 / 11953 loss=4.729, nll_loss=3.130, ppl=8.75, wps=99662, ups=2, wpb=41787.175, bsz=2651.029, num_updates=76124, lr=0.000114614, gnorm=0.805, clip=0.000, oom=0.000, loss_scale=2.000, wall=32189, train_wall=438985
| WARNING: overflow detected, setting loss scale to: 1.0
Invalid bert_input shape:torch.Size([8, 1160])
| epoch 163:   2000 / 11953 loss=4.730, nll_loss=3.130, ppl=8.76, wps=102221, ups=2, wpb=42082.888, bsz=2590.908, num_updates=77123, lr=0.00011387, gnorm=0.785, clip=0.000, oom=0.000, loss_scale=1.000, wall=32592, train_wall=439313
Invalid bert_input shape:torch.Size([8, 1167])
| epoch 163:   3000 / 11953 loss=4.730, nll_loss=3.130, ppl=8.75, wps=102857, ups=2, wpb=41922.194, bsz=2579.725, num_updates=78123, lr=0.000113139, gnorm=0.786, clip=0.000, oom=0.000, loss_scale=1.000, wall=32992, train_wall=439637
| epoch 163:   4000 / 11953 loss=4.730, nll_loss=3.130, ppl=8.76, wps=102769, ups=2, wpb=41779.462, bsz=2580.278, num_updates=79123, lr=0.000112421, gnorm=0.792, clip=0.000, oom=0.000, loss_scale=1.000, wall=33395, train_wall=439962
Invalid bert_input shape:torch.Size([24, 528])
Invalid bert_input shape:torch.Size([16, 858])
Invalid bert_input shape:torch.Size([24, 666])
| epoch 163:   5000 / 11953 loss=4.731, nll_loss=3.131, ppl=8.76, wps=103012, ups=2, wpb=41844.798, bsz=2572.173, num_updates=80123, lr=0.000111718, gnorm=0.788, clip=0.000, oom=0.000, loss_scale=1.000, wall=33800, train_wall=440278
Invalid bert_input shape:torch.Size([8, 1356])
| epoch 163:   6000 / 11953 loss=4.729, nll_loss=3.129, ppl=8.75, wps=102561, ups=2, wpb=41752.546, bsz=2568.769, num_updates=81123, lr=0.000111027, gnorm=0.788, clip=0.000, oom=0.000, loss_scale=2.000, wall=34212, train_wall=440613
Invalid bert_input shape:torch.Size([16, 840])
Invalid bert_input shape:torch.Size([8, 1524])
| epoch 163:   7000 / 11953 loss=4.730, nll_loss=3.130, ppl=8.75, wps=102406, ups=2, wpb=41792.620, bsz=2566.260, num_updates=82123, lr=0.000110349, gnorm=0.787, clip=0.000, oom=0.000, loss_scale=2.000, wall=34626, train_wall=440950
| WARNING: overflow detected, setting loss scale to: 1.0
Invalid bert_input shape:torch.Size([16, 864])
| epoch 163:   8000 / 11953 loss=4.730, nll_loss=3.130, ppl=8.75, wps=102715, ups=2, wpb=41803.038, bsz=2564.849, num_updates=83122, lr=0.000109684, gnorm=0.790, clip=0.000, oom=0.000, loss_scale=1.000, wall=35024, train_wall=441273
Invalid bert_input shape:torch.Size([16, 701])
| epoch 163:   9000 / 11953 loss=4.729, nll_loss=3.129, ppl=8.75, wps=103047, ups=2, wpb=41843.973, bsz=2563.762, num_updates=84122, lr=0.00010903, gnorm=0.791, clip=0.000, oom=0.000, loss_scale=1.000, wall=35423, train_wall=441596
Invalid bert_input shape:torch.Size([16, 843])
| epoch 163:  10000 / 11953 loss=4.728, nll_loss=3.128, ppl=8.74, wps=102891, ups=2, wpb=41874.621, bsz=2563.589, num_updates=85122, lr=0.000108387, gnorm=0.792, clip=0.000, oom=0.000, loss_scale=1.000, wall=35838, train_wall=441919
Invalid bert_input shape:torch.Size([8, 1514])
Invalid bert_input shape:torch.Size([16, 863])
| epoch 163:  11000 / 11953 loss=4.727, nll_loss=3.127, ppl=8.74, wps=103076, ups=2, wpb=41892.000, bsz=2562.137, num_updates=86122, lr=0.000107756, gnorm=0.792, clip=0.000, oom=0.000, loss_scale=1.000, wall=36239, train_wall=442242
Invalid bert_input shape:torch.Size([24, 656])
| WARNING: overflow detected, setting loss scale to: 1.0
Invalid bert_input shape:torch.Size([16, 721])
| epoch 163 | loss 4.727 | nll_loss 3.126 | ppl 8.73 | wps 102975 | ups 2 | wpb 41862.642 | bsz 2564.630 | num_updates 87073 | lr 0.000107166 | gnorm 0.792 | clip 0.000 | oom 0.000 | loss_scale 1.000 | wall 36627 | train_wall 442557
| epoch 163 | valid on 'valid' subset | loss 3.181 | nll_loss 1.350 | ppl 2.55 | num_updates 87073 | best_loss 3.18106
| saved checkpoint /scratch/project_2003453/bertnmt_data/bert-nmt_fi_sv/checkpoint163.pt (epoch 163 @ 87073 updates) (writing took 7.499174356460571 seconds)
Invalid bert_input shape:torch.Size([16, 994])
Invalid bert_input shape:torch.Size([16, 721])
| epoch 164:   1000 / 11953 loss=4.716, nll_loss=3.115, ppl=8.66, wps=104526, ups=2, wpb=41942.484, bsz=2585.806, num_updates=88074, lr=0.000106556, gnorm=0.803, clip=0.000, oom=0.000, loss_scale=1.000, wall=37165, train_wall=442891
| epoch 164:   2000 / 11953 loss=4.721, nll_loss=3.120, ppl=8.70, wps=103657, ups=2, wpb=41921.422, bsz=2565.333, num_updates=89074, lr=0.000105956, gnorm=0.789, clip=0.000, oom=0.000, loss_scale=1.000, wall=37572, train_wall=443223
Invalid bert_input shape:torch.Size([8, 1119])
Invalid bert_input shape:torch.Size([24, 638])
Invalid bert_input shape:torch.Size([24, 566])
| epoch 164:   3000 / 11953 loss=4.711, nll_loss=3.109, ppl=8.63, wps=103189, ups=2, wpb=41878.548, bsz=2569.741, num_updates=90074, lr=0.000105366, gnorm=0.785, clip=0.000, oom=0.000, loss_scale=1.000, wall=37981, train_wall=443554
| WARNING: overflow detected, setting loss scale to: 1.0
Invalid bert_input shape:torch.Size([16, 790])
| epoch 164:   4000 / 11953 loss=4.709, nll_loss=3.107, ppl=8.62, wps=103633, ups=2, wpb=41920.414, bsz=2568.252, num_updates=91073, lr=0.000104786, gnorm=0.788, clip=0.000, oom=0.000, loss_scale=1.000, wall=38381, train_wall=443877
Invalid bert_input shape:torch.Size([16, 944])
Invalid bert_input shape:torch.Size([24, 528])
| epoch 164:   5000 / 11953 loss=4.708, nll_loss=3.106, ppl=8.61, wps=103798, ups=2, wpb=41881.525, bsz=2563.856, num_updates=92073, lr=0.000104216, gnorm=0.788, clip=0.000, oom=0.000, loss_scale=1.000, wall=38780, train_wall=444201
Invalid bert_input shape:torch.Size([16, 840])
| epoch 164:   6000 / 11953 loss=4.708, nll_loss=3.106, ppl=8.61, wps=103457, ups=2, wpb=41885.096, bsz=2556.455, num_updates=93073, lr=0.000103654, gnorm=0.788, clip=0.000, oom=0.000, loss_scale=1.000, wall=39192, train_wall=444519
Invalid bert_input shape:torch.Size([16, 720])
| epoch 164:   7000 / 11953 loss=4.706, nll_loss=3.103, ppl=8.59, wps=103527, ups=2, wpb=41880.220, bsz=2557.531, num_updates=94073, lr=0.000103102, gnorm=0.786, clip=0.000, oom=0.000, loss_scale=1.000, wall=39595, train_wall=444844
Invalid bert_input shape:torch.Size([16, 782])
| WARNING: overflow detected, setting loss scale to: 1.0
| epoch 164:   8000 / 11953 loss=4.705, nll_loss=3.102, ppl=8.59, wps=103664, ups=2, wpb=41885.670, bsz=2557.167, num_updates=95072, lr=0.000102559, gnorm=0.788, clip=0.000, oom=0.000, loss_scale=1.000, wall=39995, train_wall=445170
| epoch 164:   9000 / 11953 loss=4.703, nll_loss=3.101, ppl=8.58, wps=103737, ups=2, wpb=41890.775, bsz=2556.523, num_updates=96072, lr=0.000102024, gnorm=0.789, clip=0.000, oom=0.000, loss_scale=1.000, wall=40397, train_wall=445494
Invalid bert_input shape:torch.Size([24, 595])
Invalid bert_input shape:torch.Size([16, 703])
| epoch 164:  10000 / 11953 loss=4.703, nll_loss=3.100, ppl=8.58, wps=103353, ups=2, wpb=41896.085, bsz=2561.826, num_updates=97072, lr=0.000101497, gnorm=0.789, clip=0.000, oom=0.000, loss_scale=1.000, wall=40816, train_wall=445833
Invalid bert_input shape:torch.Size([16, 896])
Invalid bert_input shape:torch.Size([16, 760])
Invalid bert_input shape:torch.Size([16, 844])
| epoch 164:  11000 / 11953 loss=4.702, nll_loss=3.100, ppl=8.57, wps=103177, ups=2, wpb=41880.364, bsz=2564.218, num_updates=98072, lr=0.000100978, gnorm=0.788, clip=0.000, oom=0.000, loss_scale=1.000, wall=41228, train_wall=446168
Invalid bert_input shape:torch.Size([16, 878])
| epoch 164 | loss 4.702 | nll_loss 3.099 | ppl 8.57 | wps 103212 | ups 2 | wpb 41863.274 | bsz 2564.963 | num_updates 99024 | lr 0.000100492 | gnorm 0.789 | clip 0.000 | oom 0.000 | loss_scale 2.000 | wall 41610 | train_wall 446482
| epoch 164 | valid on 'valid' subset | loss 3.179 | nll_loss 1.343 | ppl 2.54 | num_updates 99024 | best_loss 3.17934
| saved checkpoint /scratch/project_2003453/bertnmt_data/bert-nmt_fi_sv/checkpoint164.pt (epoch 164 @ 99024 updates) (writing took 5.7595648765563965 seconds)
Invalid bert_input shape:torch.Size([16, 864])
Invalid bert_input shape:torch.Size([16, 865])
| WARNING: overflow detected, setting loss scale to: 1.0
| epoch 165:   1000 / 11953 loss=4.681, nll_loss=3.076, ppl=8.43, wps=104133, ups=2, wpb=41692.238, bsz=2565.112, num_updates=100024, lr=9.9988e-05, gnorm=0.795, clip=0.000, oom=0.000, loss_scale=1.000, wall=42144, train_wall=446811
Invalid bert_input shape:torch.Size([24, 555])
| epoch 165:   2000 / 11953 loss=4.689, nll_loss=3.085, ppl=8.48, wps=104618, ups=2, wpb=41925.378, bsz=2565.248, num_updates=101024, lr=9.94919e-05, gnorm=0.802, clip=0.000, oom=0.000, loss_scale=1.000, wall=42545, train_wall=447133
Invalid bert_input shape:torch.Size([24, 600])
Invalid bert_input shape:torch.Size([16, 823])
Invalid bert_input shape:torch.Size([16, 863])
| epoch 165:   3000 / 11953 loss=4.686, nll_loss=3.081, ppl=8.46, wps=103679, ups=2, wpb=41772.411, bsz=2577.096, num_updates=102024, lr=9.90031e-05, gnorm=0.802, clip=0.000, oom=0.000, loss_scale=1.000, wall=42952, train_wall=447463
Invalid bert_input shape:torch.Size([16, 817])
| epoch 165:   4000 / 11953 loss=4.688, nll_loss=3.084, ppl=8.48, wps=104052, ups=2, wpb=41791.568, bsz=2568.508, num_updates=103024, lr=9.85215e-05, gnorm=0.801, clip=0.000, oom=0.000, loss_scale=1.000, wall=43350, train_wall=447785
Invalid bert_input shape:torch.Size([16, 831])
Invalid bert_input shape:torch.Size([16, 858])
Invalid bert_input shape:torch.Size([24, 573])
| epoch 165:   5000 / 11953 loss=4.688, nll_loss=3.084, ppl=8.48, wps=104035, ups=2, wpb=41784.512, bsz=2565.209, num_updates=104024, lr=9.80468e-05, gnorm=0.796, clip=0.000, oom=0.000, loss_scale=1.000, wall=43752, train_wall=448109
Invalid bert_input shape:torch.Size([16, 729])
Invalid bert_input shape:torch.Size([24, 602])
| epoch 165:   6000 / 11953 loss=4.686, nll_loss=3.082, ppl=8.47, wps=104143, ups=2, wpb=41801.072, bsz=2559.674, num_updates=105024, lr=9.75789e-05, gnorm=0.796, clip=0.000, oom=0.000, loss_scale=2.000, wall=44152, train_wall=448430
| WARNING: overflow detected, setting loss scale to: 1.0
Invalid bert_input shape:torch.Size([16, 869])
| epoch 165:   7000 / 11953 loss=4.686, nll_loss=3.081, ppl=8.46, wps=103990, ups=2, wpb=41813.785, bsz=2556.585, num_updates=106023, lr=9.71181e-05, gnorm=0.793, clip=0.000, oom=0.000, loss_scale=1.000, wall=44558, train_wall=448745
Invalid bert_input shape:torch.Size([24, 595])
Invalid bert_input shape:torch.Size([24, 616])
| epoch 165:   8000 / 11953 loss=4.685, nll_loss=3.080, ppl=8.46, wps=103997, ups=2, wpb=41845.163, bsz=2561.003, num_updates=107023, lr=9.66633e-05, gnorm=0.794, clip=0.000, oom=0.000, loss_scale=1.000, wall=44962, train_wall=449070
Invalid bert_input shape:torch.Size([8, 1562])
| epoch 165:   9000 / 11953 loss=4.684, nll_loss=3.079, ppl=8.45, wps=103996, ups=2, wpb=41873.462, bsz=2556.920, num_updates=108023, lr=9.62148e-05, gnorm=0.792, clip=0.000, oom=0.000, loss_scale=1.000, wall=45367, train_wall=449398
| epoch 165:  10000 / 11953 loss=4.685, nll_loss=3.080, ppl=8.46, wps=104022, ups=2, wpb=41882.659, bsz=2556.407, num_updates=109023, lr=9.57725e-05, gnorm=0.794, clip=0.000, oom=0.000, loss_scale=1.000, wall=45769, train_wall=449719
| WARNING: overflow detected, setting loss scale to: 1.0
| epoch 165:  11000 / 11953 loss=4.683, nll_loss=3.078, ppl=8.45, wps=104012, ups=2, wpb=41883.868, bsz=2561.608, num_updates=110022, lr=9.53367e-05, gnorm=0.793, clip=0.000, oom=0.000, loss_scale=1.000, wall=46172, train_wall=450044
Invalid bert_input shape:torch.Size([8, 1356])
| WARNING: overflow detected, setting loss scale to: 0.5
| epoch 165 | loss 4.681 | nll_loss 3.076 | ppl 8.44 | wps 104016 | ups 2 | wpb 41864.902 | bsz 2564.639 | num_updates 110973 | lr 9.49273e-05 | gnorm 0.793 | clip 0.000 | oom 0.000 | loss_scale 0.500 | wall 46553 | train_wall 450351
| epoch 165 | valid on 'valid' subset | loss 3.181 | nll_loss 1.354 | ppl 2.56 | num_updates 110973 | best_loss 3.17934
| saved checkpoint /scratch/project_2003453/bertnmt_data/bert-nmt_fi_sv/checkpoint165.pt (epoch 165 @ 110973 updates) (writing took 4.001688718795776 seconds)
Invalid bert_input shape:torch.Size([16, 729])
Invalid bert_input shape:torch.Size([16, 844])
Invalid bert_input shape:torch.Size([16, 701])
| epoch 166:   1000 / 11953 loss=4.672, nll_loss=3.066, ppl=8.37, wps=105496, ups=2, wpb=42033.513, bsz=2564.931, num_updates=111974, lr=9.45021e-05, gnorm=0.784, clip=0.000, oom=0.000, loss_scale=0.500, wall=47081, train_wall=450676
Invalid bert_input shape:torch.Size([24, 666])
| epoch 166:   2000 / 11953 loss=4.666, nll_loss=3.060, ppl=8.34, wps=103346, ups=2, wpb=41961.505, bsz=2579.282, num_updates=112974, lr=9.40829e-05, gnorm=0.793, clip=0.000, oom=0.000, loss_scale=0.500, wall=47495, train_wall=451009
Invalid bert_input shape:torch.Size([16, 869])
| epoch 166:   3000 / 11953 loss=4.667, nll_loss=3.061, ppl=8.35, wps=102529, ups=2, wpb=41889.476, bsz=2580.073, num_updates=113974, lr=9.36693e-05, gnorm=0.794, clip=0.000, oom=0.000, loss_scale=0.500, wall=47908, train_wall=451344
Invalid bert_input shape:torch.Size([16, 950])
Invalid bert_input shape:torch.Size([16, 840])
| epoch 166:   4000 / 11953 loss=4.669, nll_loss=3.063, ppl=8.36, wps=102135, ups=2, wpb=41940.950, bsz=2584.262, num_updates=114974, lr=9.3261e-05, gnorm=0.799, clip=0.000, oom=0.000, loss_scale=1.000, wall=48325, train_wall=451669
Invalid bert_input shape:torch.Size([16, 692])
Invalid bert_input shape:torch.Size([8, 1203])
| epoch 166:   5000 / 11953 loss=4.671, nll_loss=3.065, ppl=8.37, wps=102616, ups=2, wpb=41889.910, bsz=2571.135, num_updates=115974, lr=9.28581e-05, gnorm=0.797, clip=0.000, oom=0.000, loss_scale=1.000, wall=48724, train_wall=451994
| epoch 166:   6000 / 11953 loss=4.666, nll_loss=3.060, ppl=8.34, wps=103074, ups=2, wpb=41877.391, bsz=2566.315, num_updates=116974, lr=9.24603e-05, gnorm=0.795, clip=0.000, oom=0.000, loss_scale=1.000, wall=49120, train_wall=452312
Invalid bert_input shape:torch.Size([16, 837])
Invalid bert_input shape:torch.Size([16, 994])
| epoch 166:   7000 / 11953 loss=4.669, nll_loss=3.064, ppl=8.36, wps=102932, ups=2, wpb=41837.306, bsz=2569.817, num_updates=117974, lr=9.20676e-05, gnorm=0.796, clip=0.000, oom=0.000, loss_scale=1.000, wall=49528, train_wall=452643
Invalid bert_input shape:torch.Size([8, 1514])
Invalid bert_input shape:torch.Size([24, 602])
| epoch 166:   8000 / 11953 loss=4.666, nll_loss=3.060, ppl=8.34, wps=102852, ups=2, wpb=41837.476, bsz=2572.690, num_updates=118974, lr=9.16799e-05, gnorm=0.794, clip=0.000, oom=0.000, loss_scale=1.000, wall=49937, train_wall=452973
Invalid bert_input shape:torch.Size([16, 859])
| WARNING: overflow detected, setting loss scale to: 1.0
| epoch 166:   9000 / 11953 loss=4.664, nll_loss=3.058, ppl=8.33, wps=102666, ups=2, wpb=41837.308, bsz=2570.839, num_updates=119973, lr=9.12974e-05, gnorm=0.792, clip=0.000, oom=0.000, loss_scale=1.000, wall=50350, train_wall=453310
| WARNING: overflow detected, setting loss scale to: 0.5
| epoch 166:  10000 / 11953 loss=4.664, nll_loss=3.057, ppl=8.32, wps=102944, ups=2, wpb=41844.999, bsz=2566.505, num_updates=120972, lr=9.09196e-05, gnorm=0.792, clip=0.000, oom=0.000, loss_scale=0.500, wall=50747, train_wall=453632
Invalid bert_input shape:torch.Size([16, 782])
Invalid bert_input shape:torch.Size([16, 721])
Invalid bert_input shape:torch.Size([16, 831])
Invalid bert_input shape:torch.Size([24, 616])
| epoch 166:  11000 / 11953 loss=4.664, nll_loss=3.058, ppl=8.33, wps=103119, ups=2, wpb=41846.376, bsz=2563.989, num_updates=121972, lr=9.05461e-05, gnorm=0.792, clip=0.000, oom=0.000, loss_scale=0.500, wall=51146, train_wall=453954
Invalid bert_input shape:torch.Size([16, 784])
| epoch 166 | loss 4.664 | nll_loss 3.058 | ppl 8.33 | wps 103105 | ups 2 | wpb 41863.368 | bsz 2564.819 | num_updates 122924 | lr 9.01948e-05 | gnorm 0.793 | clip 0.000 | oom 0.000 | loss_scale 0.500 | wall 51535 | train_wall 454265
| epoch 166 | valid on 'valid' subset | loss 3.165 | nll_loss 1.313 | ppl 2.49 | num_updates 122924 | best_loss 3.16521
| saved checkpoint /scratch/project_2003453/bertnmt_data/bert-nmt_fi_sv/checkpoint166.pt (epoch 166 @ 122924 updates) (writing took 4.135111331939697 seconds)
Invalid bert_input shape:torch.Size([24, 656])
Invalid bert_input shape:torch.Size([16, 865])
| epoch 167:   1000 / 11953 loss=4.646, nll_loss=3.037, ppl=8.21, wps=104444, ups=2, wpb=42110.118, bsz=2533.003, num_updates=123925, lr=8.98298e-05, gnorm=0.801, clip=0.000, oom=0.000, loss_scale=0.500, wall=52080, train_wall=454610
Invalid bert_input shape:torch.Size([16, 950])
Invalid bert_input shape:torch.Size([16, 840])
| epoch 167:   2000 / 11953 loss=4.644, nll_loss=3.035, ppl=8.20, wps=104062, ups=2, wpb=41974.109, bsz=2550.549, num_updates=124925, lr=8.94696e-05, gnorm=0.784, clip=0.000, oom=0.000, loss_scale=1.000, wall=52484, train_wall=454939
Invalid bert_input shape:torch.Size([8, 1160])
Invalid bert_input shape:torch.Size([16, 843])
| epoch 167:   3000 / 11953 loss=4.644, nll_loss=3.035, ppl=8.20, wps=103575, ups=2, wpb=41931.646, bsz=2536.661, num_updates=125925, lr=8.91136e-05, gnorm=0.782, clip=0.000, oom=0.000, loss_scale=1.000, wall=52892, train_wall=455271
| epoch 167:   4000 / 11953 loss=4.644, nll_loss=3.035, ppl=8.20, wps=103461, ups=2, wpb=41953.922, bsz=2544.976, num_updates=126925, lr=8.87619e-05, gnorm=0.791, clip=0.000, oom=0.000, loss_scale=1.000, wall=53299, train_wall=455604
Invalid bert_input shape:torch.Size([16, 790])
| epoch 167:   5000 / 11953 loss=4.648, nll_loss=3.040, ppl=8.23, wps=103379, ups=2, wpb=41931.219, bsz=2541.828, num_updates=127925, lr=8.84143e-05, gnorm=0.797, clip=0.000, oom=0.000, loss_scale=1.000, wall=53705, train_wall=455928
| WARNING: overflow detected, setting loss scale to: 1.0
Invalid bert_input shape:torch.Size([8, 1524])
| epoch 167:   6000 / 11953 loss=4.647, nll_loss=3.039, ppl=8.22, wps=103352, ups=2, wpb=41890.497, bsz=2545.309, num_updates=128924, lr=8.8071e-05, gnorm=0.797, clip=0.000, oom=0.000, loss_scale=1.000, wall=54109, train_wall=456256
Invalid bert_input shape:torch.Size([16, 729])
Invalid bert_input shape:torch.Size([24, 550])
Invalid bert_input shape:torch.Size([16, 720])
| epoch 167:   7000 / 11953 loss=4.648, nll_loss=3.040, ppl=8.23, wps=103361, ups=2, wpb=41816.354, bsz=2547.479, num_updates=129924, lr=8.77315e-05, gnorm=0.797, clip=0.000, oom=0.000, loss_scale=1.000, wall=54509, train_wall=456579
Invalid bert_input shape:torch.Size([16, 758])
| epoch 167:   8000 / 11953 loss=4.647, nll_loss=3.039, ppl=8.22, wps=103528, ups=2, wpb=41852.858, bsz=2550.164, num_updates=130924, lr=8.73958e-05, gnorm=0.795, clip=0.000, oom=0.000, loss_scale=1.000, wall=54911, train_wall=456907
Invalid bert_input shape:torch.Size([24, 602])
| epoch 167:   9000 / 11953 loss=4.646, nll_loss=3.038, ppl=8.22, wps=103501, ups=2, wpb=41843.144, bsz=2556.302, num_updates=131924, lr=8.70639e-05, gnorm=0.792, clip=0.000, oom=0.000, loss_scale=1.000, wall=55315, train_wall=457235
| WARNING: overflow detected, setting loss scale to: 1.0
| epoch 167:  10000 / 11953 loss=4.648, nll_loss=3.040, ppl=8.22, wps=103200, ups=2, wpb=41833.015, bsz=2556.232, num_updates=132923, lr=8.67361e-05, gnorm=0.792, clip=0.000, oom=0.000, loss_scale=1.000, wall=55730, train_wall=457563
Invalid bert_input shape:torch.Size([16, 837])
| epoch 167:  11000 / 11953 loss=4.648, nll_loss=3.040, ppl=8.22, wps=103035, ups=2, wpb=41851.915, bsz=2563.339, num_updates=133923, lr=8.64117e-05, gnorm=0.794, clip=0.000, oom=0.000, loss_scale=1.000, wall=56144, train_wall=457896
Invalid bert_input shape:torch.Size([16, 823])
| epoch 167 | loss 4.648 | nll_loss 3.040 | ppl 8.22 | wps 103135 | ups 2 | wpb 41864.157 | bsz 2564.782 | num_updates 134875 | lr 8.61062e-05 | gnorm 0.794 | clip 0.000 | oom 0.000 | loss_scale 1.000 | wall 56528 | train_wall 458205
| epoch 167 | valid on 'valid' subset | loss 3.163 | nll_loss 1.330 | ppl 2.51 | num_updates 134875 | best_loss 3.16272
| saved checkpoint /scratch/project_2003453/bertnmt_data/bert-nmt_fi_sv/checkpoint167.pt (epoch 167 @ 134875 updates) (writing took 5.680418491363525 seconds)
Invalid bert_input shape:torch.Size([24, 627])
| epoch 168:   1000 / 11953 loss=4.614, nll_loss=3.001, ppl=8.01, wps=101235, ups=2, wpb=42037.036, bsz=2570.813, num_updates=135876, lr=8.57884e-05, gnorm=0.776, clip=0.000, oom=0.000, loss_scale=1.000, wall=57085, train_wall=458558
Invalid bert_input shape:torch.Size([16, 758])
Invalid bert_input shape:torch.Size([24, 614])
| epoch 168:   2000 / 11953 loss=4.628, nll_loss=3.018, ppl=8.10, wps=101092, ups=2, wpb=41961.537, bsz=2564.606, num_updates=136876, lr=8.54745e-05, gnorm=0.794, clip=0.000, oom=0.000, loss_scale=1.000, wall=57500, train_wall=458886
Invalid bert_input shape:torch.Size([8, 1203])
| WARNING: overflow detected, setting loss scale to: 1.0
Invalid bert_input shape:torch.Size([24, 644])
| WARNING: overflow detected, setting loss scale to: 0.5
Invalid bert_input shape:torch.Size([16, 790])
| epoch 168:   3000 / 11953 loss=4.639, nll_loss=3.030, ppl=8.17, wps=101840, ups=2, wpb=41916.490, bsz=2560.059, num_updates=137874, lr=8.51645e-05, gnorm=0.795, clip=0.000, oom=0.000, loss_scale=0.500, wall=57904, train_wall=459217
| epoch 168:   4000 / 11953 loss=4.642, nll_loss=3.034, ppl=8.19, wps=102336, ups=2, wpb=41963.301, bsz=2563.095, num_updates=138874, lr=8.48574e-05, gnorm=0.789, clip=0.000, oom=0.000, loss_scale=0.500, wall=58309, train_wall=459545
Invalid bert_input shape:torch.Size([16, 865])
| epoch 168:   5000 / 11953 loss=4.640, nll_loss=3.032, ppl=8.18, wps=102517, ups=2, wpb=41878.656, bsz=2580.362, num_updates=139874, lr=8.45535e-05, gnorm=0.791, clip=0.000, oom=0.000, loss_scale=0.500, wall=58712, train_wall=459865
Invalid bert_input shape:torch.Size([16, 944])
| epoch 168:   6000 / 11953 loss=4.636, nll_loss=3.026, ppl=8.15, wps=102667, ups=2, wpb=41844.323, bsz=2572.126, num_updates=140874, lr=8.42528e-05, gnorm=0.788, clip=0.000, oom=0.000, loss_scale=0.500, wall=59115, train_wall=460192
Invalid bert_input shape:torch.Size([16, 832])
Invalid bert_input shape:torch.Size([16, 720])
| epoch 168:   7000 / 11953 loss=4.633, nll_loss=3.024, ppl=8.13, wps=102692, ups=2, wpb=41843.857, bsz=2571.414, num_updates=141874, lr=8.39554e-05, gnorm=0.789, clip=0.000, oom=0.000, loss_scale=1.000, wall=59521, train_wall=460519
| epoch 168:   8000 / 11953 loss=4.632, nll_loss=3.023, ppl=8.13, wps=102627, ups=2, wpb=41837.763, bsz=2570.768, num_updates=142874, lr=8.36611e-05, gnorm=0.792, clip=0.000, oom=0.000, loss_scale=1.000, wall=59930, train_wall=460853
Invalid bert_input shape:torch.Size([8, 1184])
| epoch 168:   9000 / 11953 loss=4.630, nll_loss=3.021, ppl=8.12, wps=102521, ups=2, wpb=41816.212, bsz=2576.656, num_updates=143874, lr=8.33698e-05, gnorm=0.792, clip=0.000, oom=0.000, loss_scale=1.000, wall=60340, train_wall=461174
| epoch 168:  10000 / 11953 loss=4.631, nll_loss=3.022, ppl=8.12, wps=102704, ups=2, wpb=41851.885, bsz=2572.915, num_updates=144874, lr=8.30816e-05, gnorm=0.791, clip=0.000, oom=0.000, loss_scale=1.000, wall=60744, train_wall=461500
Invalid bert_input shape:torch.Size([24, 547])
| epoch 168:  11000 / 11953 loss=4.634, nll_loss=3.025, ppl=8.14, wps=102583, ups=2, wpb=41852.160, bsz=2569.707, num_updates=145874, lr=8.27963e-05, gnorm=0.792, clip=0.000, oom=0.000, loss_scale=2.000, wall=61157, train_wall=461834
| WARNING: overflow detected, setting loss scale to: 1.0
Invalid bert_input shape:torch.Size([8, 1562])
Invalid bert_input shape:torch.Size([16, 784])
Invalid bert_input shape:torch.Size([16, 795])
| epoch 168 | loss 4.634 | nll_loss 3.025 | ppl 8.14 | wps 102649 | ups 2 | wpb 41864.145 | bsz 2564.853 | num_updates 146825 | lr 8.25277e-05 | gnorm 0.793 | clip 0.000 | oom 0.000 | loss_scale 1.000 | wall 61543 | train_wall 462145
| epoch 168 | valid on 'valid' subset | loss 3.160 | nll_loss 1.327 | ppl 2.51 | num_updates 146825 | best_loss 3.15996
| saved checkpoint /scratch/project_2003453/bertnmt_data/bert-nmt_fi_sv/checkpoint168.pt (epoch 168 @ 146825 updates) (writing took 4.249779224395752 seconds)
Invalid bert_input shape:torch.Size([16, 692])
| epoch 169:   1000 / 11953 loss=4.617, nll_loss=3.006, ppl=8.03, wps=101660, ups=2, wpb=41849.974, bsz=2536.264, num_updates=147826, lr=8.22479e-05, gnorm=0.786, clip=0.000, oom=0.000, loss_scale=1.000, wall=62094, train_wall=462482
Invalid bert_input shape:torch.Size([24, 528])
| epoch 169:   2000 / 11953 loss=4.619, nll_loss=3.008, ppl=8.04, wps=103908, ups=2, wpb=42009.263, bsz=2539.758, num_updates=148826, lr=8.19711e-05, gnorm=0.800, clip=0.000, oom=0.000, loss_scale=1.000, wall=62491, train_wall=462805
| WARNING: overflow detected, setting loss scale to: 0.5
| epoch 169:   3000 / 11953 loss=4.617, nll_loss=3.005, ppl=8.03, wps=102838, ups=2, wpb=41880.188, bsz=2568.411, num_updates=149825, lr=8.16973e-05, gnorm=0.795, clip=0.000, oom=0.000, loss_scale=0.500, wall=62904, train_wall=463135
| epoch 169 | loss 4.615 | nll_loss 3.004 | ppl 8.02 | wps 103018 | ups 2 | wpb 41896.956 | bsz 2572.624 | num_updates 150000 | lr 8.16497e-05 | gnorm 0.795 | clip 0.000 | oom 0.000 | loss_scale 0.500 | wall 62973 | train_wall 463191
| epoch 169 | valid on 'valid' subset | loss 3.142 | nll_loss 1.312 | ppl 2.48 | num_updates 150000 | best_loss 3.14187
| saved checkpoint /scratch/project_2003453/bertnmt_data/bert-nmt_fi_sv/checkpoint_best.pt (epoch 169 @ 150000 updates) (writing took 6.402687311172485 seconds)
| done training in 62685.5 seconds
